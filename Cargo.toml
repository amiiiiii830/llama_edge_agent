[package]
name = "llama_edge_agent"
version = "0.1.0"
edition = "2021"
resolver = "2"

# [lib]
# path = "src/lib.rs"
# # crate-type = ["lib"]
# crate-type = ["cdylib", "lib"]

# [[bin]]
# path = "src/main.rs"
# name = "agent"

[dependencies]
lazy_static = "1.4.0"
llama-core = { version = "0.11.1", features = ["full"] }
endpoints = "0.8.0"
chat-prompts = "0.8.0"
tokio_wasi = { version = "1", features = ["full"] }
# tokio_wasi = { version = "1", features = ["macros", "rt"] }
regex = "1.10.4"
anyhow = "1"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
dotenv = "0.15.0"
# rustpython = { version = "0.3.1", default-features = false, features = [
#     "encodings",
#     "stdlib",
#     "freeze-stdlib",
# ] }
log = "0.4.21"

wasmedge-wasi-nn = "0.7.0"
clap = { version = "4.4.6", features = ["cargo"] }
once_cell = "1.18"
thiserror = "1"
futures = { version = "0.3.6", default-features = false, features = ["async-await", "std"] }
# rusqlite = { version = "0.28", features = ["bundled"] }
# libsqlite3-sys = { version = "0.25", features = ["min_sqlite_version_3_7_16", "bundled"] }
# wasmedge_wasi_socket v0.5.4
# tokio-util_wasi v0.7.5
# tokio_wasi v1.25.2
# encoding_rs v0.8.34
# regex-syntax v0.8.3
# serde_urlencoded v0.7.1
# mio_wasi v0.8.9
# tiktoken-rs v0.5.9
# hyper_wasi v0.15.2

